# app.py - Version finale avec une machine √† √©tats robuste pour la fonctionnalit√© "Retour"

import logging
from flask import Flask, request, jsonify, send_file, Response, render_template,send_from_directory,after_this_request
import requests
from flask_cors import CORS
from core_logic import generate_lesson_logic, generate_integration_logic, generate_evaluation_logic, generate_digital_lesson_logic
import os
import uuid
import re
import shutil # Import√© pour le nettoyage des dossiers
from utils import create_pdf_with_pandoc
from database import increment_stat, get_all_stats, init_db 

# On importe les dictionnaires de menus de notre code original
from bot_data import CLASSES, MATIERES, SUBSYSTEME_FR, SUBSYSTEME_EN, LANGUES_CONTENU_COMPLET, LANGUES_CONTENU_SIMPLIFIE,  REGENERATE_OPTION_FR, REGENERATE_OPTION_EN

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
app = Flask(__name__)
CORS(app)

# initialisation de la bd
init_db()

# --- Configuration pour les fichiers temporaires ---
# On s'assure que le dossier pour les t√©l√©chargements temporaires existe.
TEMP_FOLDER = '/tmp/tchatchiai_downloads'
os.makedirs(TEMP_FOLDER, exist_ok=True)


# =======================================================================
# CONSTANTES ET ARCHITECTURE DE CONVERSATION
# =======================================================================
BACK_OPTION_FR = "‚¨ÖÔ∏è Retour"
BACK_OPTION_EN = "‚¨ÖÔ∏è Back"

DATA_KEY_FOR_STEP = {
    'select_option': 'flow_type',
    'lecon_ask_subsystem': 'subsystem', 'int_ask_subsystem': 'subsystem', 'eval_ask_subsystem': 'subsystem',
    'lecon_ask_classe': 'classe', 'int_ask_classe': 'classe', 'eval_ask_classe': 'classe',
    'lecon_ask_matiere': 'matiere', 'int_ask_matiere': 'matiere', 'eval_ask_matiere': 'matiere',
    'lecon_ask_module': 'module', 'eval_ask_module': 'module',
    'lecon_ask_lecon': 'lecon',
    'lecon_ask_syllabus_method': None,
    'lecon_get_manual_syllabus': 'syllabus', 'lecon_ask_langue_contenu': 'langue_contenu',
    'int_ask_lecons': 'liste_lecons', 'int_ask_objectifs': 'objectifs_lecons', 'int_ask_langue_contenu': 'langue_contenu',
    'eval_ask_lecons': 'liste_lecons', 'eval_ask_syllabus_method': None,
    'eval_get_manual_syllabus': 'syllabus', 'eval_ask_duree_coeff': ['duree', 'coeff'],
    'eval_ask_type': 'type_epreuve', 'eval_ask_langue_contenu': 'langue_contenu',
    'digital_ask_subsystem': 'subsystem',
    'digital_ask_classe': 'classe',
    'digital_ask_matiere': 'matiere',
    'digital_ask_module': 'module',
    'digital_ask_lecon': 'lecon'
}

CONVERSATION_FLOW = {
    'select_option': {
        'question_fr': "Que souhaitez-vous faire ?", 'question_en': "What would you like to do?",
        'get_options': lambda lang, data: ["Pr√©parer une le√ßon", "Le√ßon digitalis√©e", "Produire une activit√© d'int√©gration", "Cr√©er une √©valuation"] if lang == 'fr' else ["Prepare a lesson", "Digitalised lesson","Produce an integration activity", "Create an assessment"],
        'get_next_step': lambda msg: {'le√ßon':'lecon_ask_subsystem', 'lesson':'lecon_ask_subsystem', 'digitalis√©e': 'digital_ask_subsystem', 'digitalised': 'digital_ask_subsystem', 'int√©gration':'int_ask_subsystem', 'integration':'int_ask_subsystem', '√©valuation':'eval_ask_subsystem', 'assessment':'eval_ask_subsystem'}.get(next((k for k in ['le√ßon','lesson','digitalis√©e', 'digitalised', 'int√©gration','integration','√©valuation','assessment'] if k in msg.lower()),''),None)
    },
    # --- FLUX LE√áON ---
    'lecon_ask_subsystem': {'question_fr': "Veuillez s√©lectionner le sous-syst√®me :", 'question_en': "Please select the subsystem:", 'get_options': lambda l,d: SUBSYSTEME_FR if l=='fr' else SUBSYSTEME_EN, 'get_next_step': lambda m: 'lecon_ask_classe'},
    'lecon_ask_classe': {'question_fr': "Veuillez choisir une classe :", 'question_en': "Please choose a class:", 'get_options': lambda l,d: CLASSES[l].get(d.get('subsystem'),[]), 'get_next_step': lambda m: 'lecon_ask_matiere'},
    'lecon_ask_matiere': {'question_fr': "Choisissez une mati√®re :", 'question_en': "Choose a subject:", 'get_options': lambda l,d: MATIERES[l].get(d.get('subsystem'),[]), 'get_next_step': lambda m: 'lecon_ask_module'},
    'lecon_ask_module': {'question_fr': "Quel est le titre du module ?", 'question_en': "What is the module title?", 'get_options': lambda l,d: [], 'get_next_step': lambda m: 'lecon_ask_lecon', 'is_text_input': True},
    'lecon_ask_lecon': {'question_fr': "Quel est le titre de la le√ßon ?", 'question_en': "What is the title of the lesson?", 'get_options': lambda l,d: [], 'get_next_step': lambda m: 'lecon_ask_syllabus_method', 'is_text_input': True},
    'lecon_ask_syllabus_method': {'question_fr': "Comment obtenir les informations du syllabus ?", 'question_en': "How to get syllabus info?", 'get_options': lambda l,d: ["ü§ñ Recherche Automatique (RAG)", "‚úçÔ∏è Fournir Manuellement"] if l=='fr' else ["ü§ñ Automatic Search (RAG)", "‚úçÔ∏è Provide Manually"], 'get_next_step': lambda m: 'lecon_get_manual_syllabus' if 'Manu' in m else 'lecon_ask_langue_contenu'},
    'lecon_get_manual_syllabus': {'question_fr': "D'accord, veuillez coller l'extrait du syllabus.", 'question_en': "Okay, please paste the syllabus extract.", 'get_options': lambda l,d: [], 'get_next_step': lambda m: 'lecon_ask_langue_contenu', 'is_text_input': True},
    'lecon_ask_langue_contenu': {'question_fr': "En quelle langue le contenu doit-il √™tre r√©dig√© ?", 'question_en': "In which language should the content be written?", 'get_options': lambda l,d: LANGUES_CONTENU_SIMPLIFIE if l=='en' else LANGUES_CONTENU_COMPLET, 'get_next_step': lambda m: 'pending_generation'},
   # --- FLUX ACTIVITE D'INTEGRATION ---
    'int_ask_subsystem': {'question_fr': "Veuillez s√©lectionner le sous-syst√®me :", 'question_en': "Please select the subsystem:", 'get_options': lambda l,d: SUBSYSTEME_FR if l == 'fr' else SUBSYSTEME_EN, 'get_next_step': lambda m: 'int_ask_classe'},
    'int_ask_classe': {'question_fr': "Veuillez choisir une classe :", 'question_en': "Please choose a class:", 'get_options': lambda l,d: CLASSES[l].get(d.get('subsystem'), []), 'get_next_step': lambda m: 'int_ask_matiere'},
    'int_ask_matiere': {'question_fr': "Choisissez une mati√®re :", 'question_en': "Choose a subject:", 'get_options': lambda l,d: MATIERES[l].get(d.get('subsystem'), []), 'get_next_step': lambda m: 'int_ask_lecons'},
    'int_ask_lecons': {'question_fr': "Veuillez lister les le√ßons ou th√®mes √† int√©grer.", 'question_en': "Please list the lessons or themes to integrate.", 'get_options': lambda l,d: [], 'get_next_step': lambda m: 'int_ask_objectifs', 'is_text_input': True},
    'int_ask_objectifs': {'question_fr': "Quels sont les objectifs ou concepts cl√©s √† √©valuer ?", 'question_en': "What are the key objectives or concepts to assess?", 'get_options': lambda l,d: [], 'get_next_step': lambda m: 'int_ask_langue_contenu', 'is_text_input': True},
    'int_ask_langue_contenu': {'question_fr': "En quelle langue l'activit√© doit-elle √™tre r√©dig√©e ?", 'question_en': "In which language should the activity be written?", 'get_options': lambda l,d: LANGUES_CONTENU_SIMPLIFIE if l == 'en' else LANGUES_CONTENU_COMPLET, 'get_next_step': lambda m: 'pending_generation'},
    # --- FLUX  EVALUATION ---
    'eval_ask_subsystem': {'question_fr': "Veuillez s√©lectionner le sous-syst√®me :", 'question_en': "Please select the subsystem:", 'get_options': lambda l,d: SUBSYSTEME_FR if l == 'fr' else SUBSYSTEME_EN, 'get_next_step': lambda m: 'eval_ask_classe'},
    'eval_ask_classe': {'question_fr': "Veuillez choisir une classe :", 'question_en': "Please choose a class:", 'get_options': lambda l,d: CLASSES[l].get(d.get('subsystem'), []), 'get_next_step': lambda m: 'eval_ask_matiere'},
    'eval_ask_matiere': {'question_fr': "Choisissez une mati√®re :", 'question_en': "Choose a subject:", 'get_options': lambda l,d: MATIERES[l].get(d.get('subsystem'), []), 'get_next_step': lambda m: 'eval_ask_module'},
    'eval_ask_module': { 'question_fr': "Quel est le titre du module ?", 'question_en': "What is the module title?", 'get_options': lambda l,d: [], 'get_next_step': lambda m: 'eval_ask_lecons', 'is_text_input': True },
    'eval_ask_lecons': {'question_fr': "Sur quelles le√ßons portera l'√©valuation ?", 'question_en': "Which lessons will the assessment cover?", 'get_options': lambda l,d: [], 'get_next_step': lambda m: 'eval_ask_syllabus_method', 'is_text_input': True},
    'eval_ask_syllabus_method': {'question_fr': "Comment obtenir les informations du programme ?", 'question_en': "How to get curriculum info?", 'get_options': lambda l,d: ["ü§ñ Recherche Automatique (RAG)", "‚úçÔ∏è Fournir Manuellement"] if l == 'fr' else ["ü§ñ Automatic Search (RAG)", "‚úçÔ∏è Provide Manually"], 'get_next_step': lambda msg: 'eval_get_manual_syllabus' if 'Manu' in msg else 'eval_ask_duree_coeff'},
    'eval_get_manual_syllabus': {'question_fr': "D'accord. Veuillez copier-coller l'extrait du syllabus.", 'question_en': "Alright. Please copy and paste the syllabus extract.", 'get_options': lambda l,d: [], 'get_next_step': lambda m: 'eval_ask_duree_coeff', 'is_text_input': True},
    'eval_ask_duree_coeff': {'question_fr': "Quelle est la dur√©e (ex: 1h30) et le coefficient (ex: 2) ?", 'question_en': "What is the duration (e.g., 1h 30min) and coefficient (e.g., 2)?", 'get_options': lambda l,d: [], 'get_next_step': lambda m: 'eval_ask_type', 'is_text_input': True},
    'eval_ask_type': {'question_fr': "Quel type d'√©preuve souhaitez-vous ?", 'question_en': "Which type of test would you like?", 'get_options': lambda l,d: ["Ressources + Comp√©tences", "QCM Uniquement"] if l == 'fr' else ["Resources + Competencies", "MCQ Only"], 'get_next_step': lambda m: 'eval_ask_langue_contenu'},
    'eval_ask_langue_contenu': {'question_fr': "En quelle langue l'√©preuve doit-elle √™tre r√©dig√©e ?", 'question_en': "In which language should the assessment be written?", 'get_options': lambda l,d: LANGUES_CONTENU_SIMPLIFIE if l == 'en' else LANGUES_CONTENU_COMPLET, 'get_next_step': lambda m: 'pending_generation'},
    # --- FLUX LE√áON DIGITALIS√âE ---
    'digital_ask_subsystem': {'question_fr': "Le√ßon digitalis√©e : Quel sous-syst√®me ?", 'question_en': "Digitalised lesson: Which subsystem?", 'get_options': lambda l,d: SUBSYSTEME_FR if l=='fr' else SUBSYSTEME_EN, 'get_next_step': lambda m: 'digital_ask_classe'},
    'digital_ask_classe': {'question_fr': "Veuillez choisir une classe :", 'question_en': "Please choose a class:", 'get_options': lambda l,d: CLASSES[l].get(d.get('subsystem'),[]), 'get_next_step': lambda m: 'digital_ask_matiere'},
    'digital_ask_matiere': {'question_fr': "Choisissez une mati√®re :", 'question_en': "Choose a subject:", 'get_options': lambda l,d: MATIERES[l].get(d.get('subsystem'),[]), 'get_next_step': lambda m: 'digital_ask_module'},
    'digital_ask_module': {'question_fr': "Quel est le titre du module ?", 'question_en': "What is the module title?", 'get_options': lambda l,d: [], 'get_next_step': lambda m: 'digital_ask_lecon', 'is_text_input': True},
    'digital_ask_lecon': {'question_fr': "Quel est le titre de la le√ßon √† digitaliser ?", 'question_en': "What is the title of the lesson to digitalise?", 'get_options': lambda l,d: [], 'get_next_step': lambda m: 'pending_generation', 'is_text_input': True}
}

def handle_chat_recursive(state, message):
    fake_data = {'message': message, 'state': state}
    with app.test_request_context('/api/chat', method='POST', json=fake_data):
        return handle_chat()
    


@app.route('/api/chat', methods=['POST'])
def handle_chat():
    """
    G√®re toute la logique de conversation du chatbot.
    Cette fonction est con√ßue comme une machine √† √©tats qui traite la demande de l'utilisateur
    en suivant un ordre de priorit√© clair pour √©viter les erreurs.
    """
    # --- PHASE 0 : INITIALISATION ---
    # On r√©cup√®re les donn√©es de la requ√™te et on initialise nos variables de travail.
    data = request.get_json()
    user_message = data.get('message')
    state = data.get('state', {})
    
    current_step = state.get('currentStep', 'start')
    lang = state.get('lang', 'en')
    collected_data = state.get('collectedData', {})
    step_history = state.get('step_history', [])
    
    # =======================================================================
    # --- PHASE 1 : GESTION DES ACTIONS SP√âCIALES ---
    # Ces actions (Retour, R√©g√©n√©rer, etc.) ont la priorit√© sur le flux normal.
    # =======================================================================

    # Action 1: L'utilisateur veut r√©g√©n√©rer la derni√®re ressource.
    if user_message in [REGENERATE_OPTION_FR, REGENERATE_OPTION_EN]:
        # La solution la plus simple : on force l'√©tape √† "generation_step".
        # Le code continuera son ex√©cution et entrera dans le bloc de g√©n√©ration plus bas,
        # en utilisant les "collected_data" qui sont d√©j√† en m√©moire.
        current_step = 'generation_step'

    # Action 2: Le frontend nous informe que la conversion PDF a √©chou√©.
    elif user_message == "internal_pdf_generation_failed":
        response_text = "D√©sol√©, la conversion en PDF a √©chou√©. Cela est souvent d√ª √† des caract√®res sp√©ciaux inattendus. Vous pouvez essayer de r√©g√©n√©rer le contenu." if lang == 'fr' else "Sorry, the PDF conversion failed. This is often due to unexpected special characters. You can try regenerating the content."
        options = [REGENERATE_OPTION_FR, "Recommencer"] if lang == 'fr' else [REGENERATE_OPTION_EN, "Restart"]
        # On retourne directement une r√©ponse, car c'est une action terminale.
        return jsonify({'response': response_text, 'options': options, 'state': state})
    
    # Action 3: L'utilisateur veut revenir √† l'√©tape pr√©c√©dente.
    elif user_message in [BACK_OPTION_FR, BACK_OPTION_EN]:
        if not step_history: # S'il n'y a pas d'historique, on recommence.
            state = {'lang': lang, 'currentStep': 'select_option', 'collectedData': {}, 'step_history': []}
        else: # Sinon, on d√©pile la derni√®re √©tape.
            step_to_revert_to = step_history.pop()
            # On retire la donn√©e collect√©e √† cette √©tape pour pouvoir la redemander.
            data_key_to_remove = DATA_KEY_FOR_STEP.get(step_to_revert_to)
            if data_key_to_remove:
                if isinstance(data_key_to_remove, list):
                    for key in data_key_to_remove: collected_data.pop(key, None)
                else:
                    collected_data.pop(data_key_to_remove, None)
            # On met √† jour l'√©tat pour refl√©ter le retour en arri√®re.
            state['currentStep'] = step_to_revert_to
            state['collectedData'] = collected_data
            state['step_history'] = step_history
        # On demande √† la fonction d'afficher l'√©tape pr√©c√©dente.
        return handle_chat_recursive(state, "internal_show_step")

    # Action 4: L'utilisateur veut tout recommencer ou a fini un t√©l√©chargement.
    elif user_message in ["Recommencer", "Restart", "internal_pdf_download_complete"]:
        state = {'lang': lang, 'currentStep': 'select_option', 'collectedData': {}, 'step_history': []}
        # On demande √† la fonction d'afficher la toute premi√®re option.
        return handle_chat_recursive(state, "internal_show_step")
    
    # =======================================================================
    # --- PHASE 2 : GESTION DES √âTAPES DE CONTR√îLE DU FLUX ---
    # Ces √©tapes ne collectent pas de donn√©es mais dirigent la conversation.
    # =======================================================================

    # √âtape 'start': Choix de la langue, la toute premi√®re interaction.
    if current_step == 'start':
        lang = 'fr' if 'Fran√ßais' in user_message else 'en'
        state = {'lang': lang, 'currentStep': 'select_option', 'collectedData': {}, 'step_history': []}
        return handle_chat_recursive(state, "internal_show_step")
    
    # √âtape 'pending_generation': La collecte de donn√©es est termin√©e, on passe √† la g√©n√©ration.
    if current_step == 'pending_generation':
        # On met √† jour la variable locale pour que le code entre dans le bloc de g√©n√©ration ci-dessous.
        current_step = 'generation_step'

    # =======================================================================
    # --- PHASE 3 : LE BLOC DE G√âN√âRATION ---
    # Si l'√©tape est 'generation_step', on appelle l'IA. C'est un bloc terminal.
    # =======================================================================
    if current_step == 'generation_step':
        flow_type = collected_data.get('flow_type')
        try:
            # S√©curit√© : si on arrive ici sans savoir quoi faire, on l√®ve une erreur.
            if not flow_type:
                raise ValueError("flow_type est manquant dans collected_data. Impossible de g√©n√©rer.")
            
            # Logique de pr√©paration des arguments et d'appel √† l'IA (inchang√©e).
            # ...
            generated_text = ""
            lesson_args = {k: v for k, v in collected_data.items() if k in ['classe', 'matiere', 'module', 'lecon', 'syllabus', 'langue_contenu']}
            integration_args = {k: v for k, v in collected_data.items() if k in ['classe', 'matiere', 'liste_lecons', 'objectifs_lecons', 'langue_contenu']}
            evaluation_args = {k: v for k, v in collected_data.items() if k in ['classe', 'matiere', 'liste_lecons', 'duree', 'coeff', 'langue_contenu', 'type_epreuve_key', 'contexte_syllabus']}
            digital_args = {k: v for k, v in collected_data.items() if k in ['classe', 'matiere', 'module', 'lecon', 'langue_contenu']}

            if flow_type == 'lecon':
                generated_text, _ = generate_lesson_logic(**lesson_args)
                increment_stat('lessons_generated')
            elif flow_type == 'digital':
                generated_text, _ = generate_digital_lesson_logic(**digital_args)
                increment_stat('digital_lessons_generated')
            elif flow_type == 'integration':
                 generated_text, _ = generate_integration_logic(**integration_args)
                 increment_stat('integrations_generated')
            elif flow_type == 'evaluation':
                user_choice = collected_data.get('type_epreuve', '')
                collected_data['type_epreuve_key'] = "junior_mcq" if 'QCM' in user_choice else "junior_resources_competencies"
                collected_data['contexte_syllabus'] = collected_data.get('syllabus', "Non fourni.")
                evaluation_args['type_epreuve_key'] = collected_data['type_epreuve_key']
                evaluation_args['contexte_syllabus'] = collected_data['contexte_syllabus']
                args_to_send = {k: v for k, v in collected_data.items() if k in evaluation_args}
                generated_text, _ = generate_evaluation_logic(**args_to_send)
                increment_stat('evaluations_generated')
            
            increment_stat('total_documents')
            response_text = generated_text
            
            # Pr√©paration des options apr√®s la g√©n√©ration (avec "R√©g√©n√©rer").
            if flow_type == 'digital':
                 options_fr = ["Recommencer", REGENERATE_OPTION_FR, "T√©l√©charger en Pr√©sentation (PDF)"]
                 options_en = ["Restart", REGENERATE_OPTION_EN, "Download as Presentation (PDF)"]
            else:
                 options_fr = ["Recommencer", REGENERATE_OPTION_FR, "T√©l√©charger en PDF"]
                 options_en = ["Restart", REGENERATE_OPTION_EN, "Download PDF"]
            options = options_fr if lang == 'fr' else options_en
            
            # Mise √† jour finale de l'√©tat avant de renvoyer la r√©ponse.
            state['generated_text'] = generated_text
            state['step_history'] = [] # On vide l'historique car le flux est termin√©.
            
        except Exception as e:
            logging.error(f"ERREUR LORS DE LA G√âN√âRATION (flow: {flow_type}): {e}")
            response_text = "D√©sol√©, une erreur est survenue." if lang == 'fr' else "Sorry, an error occurred."
            options = ["Recommencer"] if lang == 'fr' else ["Restart"]
            state = {'lang': lang, 'currentStep': 'select_option', 'collectedData': {}, 'step_history': []}
        
        return jsonify({'response': response_text, 'options': options, 'state': state})

    # =======================================================================
    # --- PHASE 4 : GESTION DU FLUX DE CONVERSATION NORMAL ---
    # Si aucune des conditions ci-dessus n'est remplie, on suit la carte de conversation.
    # =======================================================================

    # On r√©cup√®re la d√©finition de l'√©tape actuelle.
    step_definition = CONVERSATION_FLOW.get(current_step)
    if not step_definition: # Si l'√©tape est inconnue, on recommence.
        state = {'lang': lang, 'currentStep': 'select_option', 'collectedData': {}, 'step_history': []}
        return handle_chat_recursive(state, "internal_show_step")

    # Si le message n'est pas "internal_show_step", cela signifie que l'utilisateur a r√©pondu.
    if user_message != "internal_show_step":
        # A. On collecte la donn√©e fournie par l'utilisateur.
        if current_step == 'select_option':
            # CORRECTION CRUCIALE : On stocke 'flow_type' dans 'collected_data'.
            if 'digital' in user_message.lower(): collected_data['flow_type'] = 'digital'
            elif 'le√ßon' in user_message.lower() or 'lesson' in user_message.lower(): collected_data['flow_type'] = 'lecon'
            elif 'int√©gration' in user_message.lower() or 'integration' in user_message.lower(): collected_data['flow_type'] = 'integration'
            elif '√©valuation' in user_message.lower() or 'assessment' in user_message.lower(): collected_data['flow_type'] = 'evaluation'
        else:
            # Pour toutes les autres √©tapes, on utilise notre dictionnaire de mapping.
            data_key = DATA_KEY_FOR_STEP.get(current_step)
            if data_key:
                if isinstance(data_key, list):
                    parts = user_message.split(',')
                    collected_data[data_key[0]] = parts[0].strip() if parts else "N/A"
                    collected_data[data_key[1]] = parts[1].strip() if len(parts) > 1 else "N/A"
                elif data_key == 'subsystem':
                    collected_data[data_key] = 'esg' if 'g√©n√©ral' in user_message.lower() or 'general' in user_message.lower() else 'est'
                else:
                    collected_data[data_key] = user_message

        # B. On d√©termine l'√©tape suivante.
        next_step_func = step_definition.get('get_next_step')
        next_step = next_step_func(user_message) if next_step_func else None
        if not next_step: # Si la r√©ponse n'est pas comprise.
            response_text = "Je n'ai pas compris, veuillez r√©essayer." if lang == 'fr' else "I didn't understand, please try again."
            options = step_definition['get_options'](lang, collected_data)
            return jsonify({'response': response_text, 'options': options, 'state': state})
        
        # C. On met √† jour l'√©tat COMPLETEMENT avant l'appel r√©cursif.
        step_history.append(current_step)
        state['currentStep'] = next_step
        state['collectedData'] = collected_data
        state['step_history'] = step_history

        # D. On demande √† la fonction d'afficher la nouvelle √©tape.
        return handle_chat_recursive(state, "internal_show_step")
    
    # Si on arrive ici, cela signifie que le message √©tait "internal_show_step".
    # On se contente donc d'afficher la question et les options de l'√©tape actuelle.
    response_text = step_definition['question_fr'] if lang == 'fr' else step_definition['question_en']
    options = list(step_definition['get_options'](lang, collected_data))
    is_text_input = step_definition.get('is_text_input', False)
    if step_history:
        options.insert(0, BACK_OPTION_FR if lang == 'fr' else BACK_OPTION_EN)
    
    return jsonify({'response': response_text, 'options': options, 'is_text_input': is_text_input, 'state': state})

#generation pdf
#la fonction handle_generate_pdf

# =======================================================================
# NOUVELLE ARCHITECTURE DE T√âL√âCHARGEMENT EN 2 √âTAPES
# =======================================================================

# --- √âTAPE 1 : G√©n√©ration du PDF ---
@app.route('/api/generate-pdf', methods=['POST'])
def handle_generate_pdf():
    """
    Cette fonction ne fait qu'une chose : elle g√©n√®re le PDF, le sauvegarde
    sur le serveur, et renvoie le nom du fichier √† t√©l√©charger au frontend.
    """
    data = request.get_json()
    if not data or 'markdown_text' not in data or 'state' not in data:
        return jsonify({"error": "Donn√©es manquantes."}), 400

    markdown_text = data.get('markdown_text')
    state = data.get('state')
    doc_type = state.get('flow_type', 'document')
    lang = state.get('lang', 'en')
    collected_data = state.get('collectedData', {})

      # On r√©cup√®re 'flow_type' depuis 'collectedData' et non plus depuis 'state'.
    doc_type = collected_data.get('flow_type', 'document')

    # --- Logique de nommage du fichier final (pour l'utilisateur) ---
    def sanitize_title(title_str):
        if not title_str: return "Sans_Titre"
        return re.sub(r'[^\w\s-]', '', title_str).strip().replace(' ', '_')

    prefix_map = {
        ('lecon', 'fr'): "lecon", ('lecon', 'en'): "lesson",
        ('digital', 'fr'): "lecon_digitalisee", ('digital', 'en'): "digital_lesson",
        ('integration', 'fr'): "activite_integration", ('integration', 'en'): "integration_activity",
        ('evaluation', 'fr'): "evaluation", ('evaluation', 'en'): "assessment"
    }
    prefix = prefix_map.get((doc_type, lang), "document")
    title = collected_data.get('lecon') or collected_data.get('module') or "Sans_Titre"
    final_download_name = f"tchatchiai_{prefix}_{sanitize_title(title)}.pdf"

    # On g√©n√®re un nom de fichier temporaire unique pour le stockage sur le serveur
    temp_filename = f"{uuid.uuid4()}.pdf"
    temp_filepath = os.path.join(TEMP_FOLDER, temp_filename)

    try:
        lang_code = state.get('lang', 'en').lower()
        output_format = 'beamer' if doc_type == 'digital' else 'pdf'

        pdf_success = create_pdf_with_pandoc(
            text=markdown_text, filename=temp_filepath, lang_contenu_code=lang_code,
            doc_type=doc_type, output_format=output_format
        )
        if not pdf_success:
            raise Exception("La conversion Pandoc/LaTeX a √©chou√©.")

        # Au lieu d'envoyer le fichier, on envoie une r√©ponse JSON avec les noms
        return jsonify({
            "success": True,
            "temp_filename": temp_filename, # Le nom unique sur le serveur
            "download_filename": final_download_name # Le nom final pour l'utilisateur
        })

    except Exception as e:
        logging.error(f"Erreur lors de la cr√©ation du PDF : {e}")
        return jsonify({"error": "Une erreur interne est survenue.", "details": str(e)}), 500


# --- √âTAPE 2 : T√©l√©chargement et Nettoyage ---
@app.route('/api/download/<temp_filename>/<download_filename>', methods=['GET'])
def download_and_cleanup_file(temp_filename, download_filename):
    """
    Cette fonction sert le fichier demand√© avec le bon nom de t√©l√©chargement,
    puis programme sa suppression.
    """
    try:
        # On construit le chemin complet vers le fichier temporaire
        filepath = os.path.join(TEMP_FOLDER, temp_filename)
        
        if not os.path.exists(filepath):
            return jsonify({"error": "Fichier non trouv√© ou d√©j√† supprim√©."}), 404

        # On pr√©pare la r√©ponse en envoyant le fichier
        response = send_file(filepath, as_attachment=True)
        
        # **LA CORRECTION EST ICI**
        # On force l'en-t√™te 'Content-Disposition' pour que le navigateur utilise le bon nom de fichier.
        # C'est la m√©thode la plus fiable.
        response.headers["Content-Disposition"] = f"attachment; filename=\"{download_filename}\""
        
        # On programme le nettoyage du fichier APRES l'envoi de la r√©ponse
        @after_this_request
        def cleanup(response):
            try:
                os.remove(filepath)
                logging.info(f"Fichier temporaire supprim√© : {temp_filename}")
            except Exception as e:
                logging.error(f"Erreur lors de la suppression du fichier temporaire {temp_filename}: {e}")
            return response
            
        return response

    except Exception as e:
        logging.error(f"Erreur inattendue dans la route de t√©l√©chargement: {e}")
        return jsonify({"error": "Une erreur de serveur est survenue lors du t√©l√©chargement."}), 500




# =======================================================================
# ROUTES FOR STATS
# =======================================================================

@app.route('/api/stats')
def get_stats():
    """Endpoint pour r√©cup√©rer les statistiques d'utilisation."""
    # get_all_stats() already returns the dictionary in the correct format.
    # We can just return it directly.
    stats = get_all_stats()
    return jsonify(stats)
# =======================================================================
# ROUTES FOR STATIC PAGES
# =======================================================================

@app.route('/')
def index():
    """Serve the main application page"""
    return render_template('index.html')

@app.route('/about')
def about_page():
    """Route for the About page"""
    return render_template('about.html')

@app.route('/donate')
def donate_page():
    """Route for the Donate page"""
    return render_template('donate.html')

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=True)